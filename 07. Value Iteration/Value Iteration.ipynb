{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AtmQ-3qFoiiB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define possible actions in the environment\n",
        "ACTION_SPACE = ('U', 'D', 'L', 'R')  # U: Up, D: Down, L: Left, R: Right\n",
        "\n",
        "class Grid:  # Environment representing a grid world\n",
        "  def __init__(self, rows, cols, start):\n",
        "    \"\"\"\n",
        "    Initialize the grid environment with the given dimensions and starting position.\n",
        "    :param rows: Number of rows in the grid\n",
        "    :param cols: Number of columns in the grid\n",
        "    :param start: Starting position (i, j) for the agent\n",
        "    \"\"\"\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    \"\"\"\n",
        "    Set the rewards and possible actions for each cell in the grid.\n",
        "    :param rewards: Dictionary where keys are (i, j) positions and values are rewards\n",
        "    :param actions: Dictionary where keys are (i, j) positions and values are lists of allowed actions\n",
        "    \"\"\"\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    \"\"\"\n",
        "    Set the agent's position in the grid to a specific state.\n",
        "    :param s: Tuple (i, j) representing the agent's new position\n",
        "    \"\"\"\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    \"\"\"\n",
        "    Returns the current state (position) of the agent.\n",
        "    :return: Tuple (i, j) representing the agent's current position\n",
        "    \"\"\"\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    \"\"\"\n",
        "    Check if a state is terminal, meaning there are no actions available from this state.\n",
        "    :param s: Tuple (i, j) representing a state\n",
        "    :return: True if the state is terminal, False otherwise\n",
        "    \"\"\"\n",
        "    return s not in self.actions\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the agent to the starting position.\n",
        "    :return: Tuple (i, j) representing the agent's start position\n",
        "    \"\"\"\n",
        "    self.i = 2\n",
        "    self.j = 0\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def get_next_state(self, s, a):\n",
        "    \"\"\"\n",
        "    Get the next state given a current state and an action.\n",
        "    :param s: Current state (i, j) as a tuple\n",
        "    :param a: Action to be taken ('U', 'D', 'L', 'R')\n",
        "    :return: Tuple (new_i, new_j) representing the next state after taking action\n",
        "    \"\"\"\n",
        "    i, j = s[0], s[1]\n",
        "\n",
        "    # Update position based on the action, if the action is allowed from the current state\n",
        "    if a in self.actions[(i, j)]:\n",
        "      if a == 'U':\n",
        "        i -= 1\n",
        "      elif a == 'D':\n",
        "        i += 1\n",
        "      elif a == 'R':\n",
        "        j += 1\n",
        "      elif a == 'L':\n",
        "        j -= 1\n",
        "    return i, j\n",
        "\n",
        "  def move(self, action):\n",
        "    \"\"\"\n",
        "    Move the agent in the specified direction, if the action is allowed from the current position.\n",
        "    :param action: Action to take ('U', 'D', 'L', 'R')\n",
        "    :return: Reward received after taking the action\n",
        "    \"\"\"\n",
        "    # Check if the action is valid in the current state\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    # Return the reward for the new position, defaulting to 0 if no reward is defined\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    \"\"\"\n",
        "    Undo the last move, moving the agent in the opposite direction of the action.\n",
        "    :param action: Action to undo ('U', 'D', 'L', 'R')\n",
        "    \"\"\"\n",
        "    # Move in the opposite direction of the specified action\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    # Ensure the state after undoing is valid\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    \"\"\"\n",
        "    Check if the game is over, which is true if the agent is in a terminal state.\n",
        "    :return: True if in a terminal state, False otherwise\n",
        "    \"\"\"\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    \"\"\"\n",
        "    Get all possible states in the grid, defined as any position with actions or rewards.\n",
        "    :return: Set of tuples representing all possible states\n",
        "    \"\"\"\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid():\n",
        "  \"\"\"\n",
        "  Define a standard 3x4 grid with rewards and actions.\n",
        "  Layout:\n",
        "    .  .  .  1\n",
        "    .  x  . -1\n",
        "    s  .  .  .\n",
        "  Legend:\n",
        "    - s: Starting position\n",
        "    - x: Blocked position (no actions allowed)\n",
        "    - Numbers: Rewards at certain states\n",
        "\n",
        "  :return: An instance of the Grid class with rewards and actions set\n",
        "  \"\"\"\n",
        "  g = Grid(3, 4, (2, 0))  # 3x4 grid with start position at (2, 0)\n",
        "\n",
        "  # Define rewards for reaching specific states\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "\n",
        "  # Define possible actions from each state\n",
        "  actions = {\n",
        "    (0, 0): ('D', 'R'),  # Can go Down or Right from (0, 0)\n",
        "    (0, 1): ('L', 'R'),  # Can go Left or Right from (0, 1)\n",
        "    (0, 2): ('L', 'D', 'R'),  # Can go Left, Down, or Right from (0, 2)\n",
        "    (1, 0): ('U', 'D'),  # Can go Up or Down from (1, 0)\n",
        "    (1, 2): ('U', 'D', 'R'),  # Can go Up, Down, or Right from (1, 2)\n",
        "    (2, 0): ('U', 'R'),  # Can go Up or Right from (2, 0)\n",
        "    (2, 1): ('L', 'R'),  # Can go Left or Right from (2, 1)\n",
        "    (2, 2): ('L', 'R', 'U'),  # Can go Left, Right, or Up from (2, 2)\n",
        "    (2, 3): ('L', 'U'),  # Can go Left or Up from (2, 3)\n",
        "  }\n",
        "\n",
        "  # Set rewards and actions in the grid\n",
        "  g.set(rewards, actions)\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WindyGrid:\n",
        "    def __init__(self, rows, cols, start):\n",
        "        \"\"\"\n",
        "        Initialize the grid with given dimensions and starting position.\n",
        "\n",
        "        Parameters:\n",
        "        rows (int): Number of rows in the grid.\n",
        "        cols (int): Number of columns in the grid.\n",
        "        start (tuple): Starting position (row, col).\n",
        "        \"\"\"\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.i = start[0]  # Row position of the agent\n",
        "        self.j = start[1]  # Column position of the agent\n",
        "\n",
        "    def set(self, rewards, actions, probs):\n",
        "        \"\"\"\n",
        "        Set the rewards, actions, and transition probabilities for each state.\n",
        "\n",
        "        Parameters:\n",
        "        rewards (dict): Dictionary mapping each state (i, j) to its reward.\n",
        "        actions (dict): Dictionary mapping each state (i, j) to available actions.\n",
        "        probs (dict): Dictionary defining the transition probabilities p(s' | s, a).\n",
        "        \"\"\"\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "        self.probs = probs\n",
        "\n",
        "    def set_state(self, s):\n",
        "        \"\"\"\n",
        "        Set the agent's current state.\n",
        "\n",
        "        Parameters:\n",
        "        s (tuple): The state to set (i, j).\n",
        "        \"\"\"\n",
        "        self.i = s[0]\n",
        "        self.j = s[1]\n",
        "\n",
        "    def current_state(self):\n",
        "        \"\"\"\n",
        "        Return the agent's current state.\n",
        "\n",
        "        Returns:\n",
        "        tuple: The current state (i, j).\n",
        "        \"\"\"\n",
        "        return (self.i, self.j)\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        \"\"\"\n",
        "        Check if a given state is terminal (no actions available).\n",
        "\n",
        "        Parameters:\n",
        "        s (tuple): The state to check.\n",
        "\n",
        "        Returns:\n",
        "        bool: True if the state is terminal, False otherwise.\n",
        "        \"\"\"\n",
        "        return s not in self.actions\n",
        "\n",
        "    def move(self, action):\n",
        "        \"\"\"\n",
        "        Attempt to move the agent in the specified direction, using transition probabilities.\n",
        "\n",
        "        Parameters:\n",
        "        action (str): The action to take ('U', 'D', 'L', or 'R').\n",
        "\n",
        "        Returns:\n",
        "        int: The reward associated with the resulting state (if any).\n",
        "        \"\"\"\n",
        "        # Get the current state\n",
        "        s = (self.i, self.j)\n",
        "\n",
        "        # Get the transition probabilities for the action\n",
        "        next_state_probs = self.probs[(s, action)]\n",
        "        next_states = list(next_state_probs.keys())    # Possible next states\n",
        "        next_probs = list(next_state_probs.values())   # Corresponding probabilities\n",
        "\n",
        "        # Choose the next state based on the defined probabilities\n",
        "        next_state_idx = np.random.choice(len(next_states), p=next_probs)\n",
        "        s2 = next_states[next_state_idx]\n",
        "\n",
        "        # Update the agent's current position\n",
        "        self.i, self.j = s2\n",
        "\n",
        "        # Return the reward for the resulting state (if any)\n",
        "        return self.rewards.get(s2, 0)\n",
        "\n",
        "    def game_over(self):\n",
        "        \"\"\"\n",
        "        Check if the game is over (if the agent is in a terminal state).\n",
        "\n",
        "        Returns:\n",
        "        bool: True if the game is over, False otherwise.\n",
        "        \"\"\"\n",
        "        return (self.i, self.j) not in self.actions\n",
        "\n",
        "    def all_states(self):\n",
        "        \"\"\"\n",
        "        Get a set of all possible states, including states with rewards and states with available actions.\n",
        "\n",
        "        Returns:\n",
        "        set: A set containing all states in the grid.\n",
        "        \"\"\"\n",
        "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def windy_grid():\n",
        "    \"\"\"\n",
        "    Create a specific instance of the WindyGrid with predefined rewards, actions, and transition probabilities.\n",
        "\n",
        "    Returns:\n",
        "    WindyGrid: An instance of the WindyGrid class.\n",
        "    \"\"\"\n",
        "    # Initialize the grid with 3 rows, 4 columns, and a starting position at (2, 0)\n",
        "    g = WindyGrid(3, 4, (2, 0))\n",
        "\n",
        "    # Define rewards for certain states\n",
        "    rewards = {(0, 3): 1, (1, 3): -1}  # Positive reward at (0, 3), negative reward at (1, 3)\n",
        "\n",
        "    # Define actions available at each non-terminal state\n",
        "    actions = {\n",
        "        (0, 0): ('D', 'R'),\n",
        "        (0, 1): ('L', 'R'),\n",
        "        (0, 2): ('L', 'D', 'R'),\n",
        "        (1, 0): ('U', 'D'),\n",
        "        (1, 2): ('U', 'D', 'R'),\n",
        "        (2, 0): ('U', 'R'),\n",
        "        (2, 1): ('L', 'R'),\n",
        "        (2, 2): ('L', 'R', 'U'),\n",
        "        (2, 3): ('L', 'U'),\n",
        "    }\n",
        "\n",
        "    # Define transition probabilities p(s' | s, a)\n",
        "    # Each entry specifies possible outcomes when performing an action in a given state\n",
        "    probs = {\n",
        "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
        "        ((2, 0), 'D'): {(2, 0): 1.0},\n",
        "        ((2, 0), 'L'): {(2, 0): 1.0},\n",
        "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
        "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
        "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
        "        ((1, 0), 'L'): {(1, 0): 1.0},\n",
        "        ((1, 0), 'R'): {(1, 0): 1.0},\n",
        "        ((0, 0), 'U'): {(0, 0): 1.0},\n",
        "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
        "        ((0, 0), 'L'): {(0, 0): 1.0},\n",
        "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
        "        ((0, 1), 'U'): {(0, 1): 1.0},\n",
        "        ((0, 1), 'D'): {(0, 1): 1.0},\n",
        "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
        "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
        "        ((0, 2), 'U'): {(0, 2): 1.0},\n",
        "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
        "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
        "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
        "        ((2, 1), 'U'): {(2, 1): 1.0},\n",
        "        ((2, 1), 'D'): {(2, 1): 1.0},\n",
        "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
        "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
        "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
        "        ((2, 2), 'D'): {(2, 2): 1.0},\n",
        "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
        "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
        "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
        "        ((2, 3), 'D'): {(2, 3): 1.0},\n",
        "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
        "        ((2, 3), 'R'): {(2, 3): 1.0},\n",
        "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},  # Probabilistic transition\n",
        "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
        "        ((1, 2), 'L'): {(1, 2): 1.0},\n",
        "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
        "    }\n",
        "\n",
        "    # Set the rewards, actions, and transition probabilities in the grid\n",
        "    g.set(rewards, actions, probs)\n",
        "\n",
        "    # Return the configured grid instance\n",
        "    return g"
      ],
      "metadata": {
        "id": "MUd32bkRpwxg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convergence threshold for value iteration\n",
        "convergence_threshold = 1e-3\n",
        "\n",
        "# Define possible actions in the grid world\n",
        "ACTION_SPACE = ('U', 'D', 'L', 'R')  # U: Up, D: Down, L: Left, R: Right\n",
        "\n",
        "def print_values(V, g):\n",
        "    \"\"\"\n",
        "    Print the value function in a grid layout.\n",
        "    :param V: Dictionary mapping each state to its value.\n",
        "    :param g: Grid object, used for dimensions and layout.\n",
        "    \"\"\"\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            v = V.get((i, j), 0)  # Get the value for each state, default to 0 if not in V\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")  # Format for positive values\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\")  # Format for negative values\n",
        "        print(\"\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "def print_policy(P, g):\n",
        "    \"\"\"\n",
        "    Print the policy in a grid layout.\n",
        "    :param P: Dictionary mapping each state to the optimal action under the policy.\n",
        "    :param g: Grid object, used for dimensions and layout.\n",
        "    \"\"\"\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            a = P.get((i, j), ' ')  # Get the action for each state, default to blank if not in policy\n",
        "            print(\"  %s  |\" % a, end=\"\")\n",
        "        print(\"\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "def get_transition_probs_and_rewards(grid):\n",
        "    \"\"\"\n",
        "    Extract the transition probabilities and rewards for both deterministic and probabilistic grids.\n",
        "    :param grid: Grid object, could be standard_grid or windy_grid.\n",
        "    :return: transition_probs and rewards dictionaries.\n",
        "    \"\"\"\n",
        "    transition_probs = {}  # Dictionary to store transition probabilities\n",
        "    rewards = {}           # Dictionary to store rewards for each state-action-next_state\n",
        "\n",
        "    # Determine if the grid is probabilistic by checking for the 'probs' attribute\n",
        "    is_probabilistic = hasattr(grid, 'probs')\n",
        "\n",
        "    for s in grid.all_states():\n",
        "        if not grid.is_terminal(s):\n",
        "            for a in ACTION_SPACE:\n",
        "                # For probabilistic environments (WindyGrid), use predefined probabilities\n",
        "                if is_probabilistic:\n",
        "                    if (s, a) in grid.probs:\n",
        "                        transition_probs[(s, a)] = grid.probs[(s, a)]\n",
        "                else:\n",
        "                    # For deterministic environments, assume a single transition with probability 1\n",
        "                    s2 = grid.get_next_state(s, a)\n",
        "                    transition_probs[(s, a)] = {s2: 1.0}\n",
        "\n",
        "                # Record rewards if defined in the grid's reward structure\n",
        "                for next_state in transition_probs[(s, a)]:\n",
        "                    if next_state in grid.rewards:\n",
        "                        rewards[(s, a, next_state)] = grid.rewards[next_state]\n",
        "\n",
        "    return transition_probs, rewards\n",
        "\n",
        "def value_iteration(gamma, grid_fn, show_iteration=False):\n",
        "    \"\"\"\n",
        "    Perform value iteration to find the optimal value function and policy for deterministic and probabilistic grids.\n",
        "    :param gamma: Discount factor for future rewards.\n",
        "    :param grid_fn: Function to initialize the grid (either standard_grid or windy_grid).\n",
        "    :param show_iteration: If True, display the value function after each iteration.\n",
        "    :return: Final value function and optimal policy after convergence.\n",
        "    \"\"\"\n",
        "    # Initialize the grid using the provided function (standard_grid or windy_grid)\n",
        "    grid = grid_fn()\n",
        "    transition_probs, rewards = get_transition_probs_and_rewards(grid)\n",
        "\n",
        "    # Initialize value function V for all states\n",
        "    V = {s: 0 for s in grid.all_states()}\n",
        "\n",
        "    # Print rewards in the grid layout\n",
        "    print(\"Rewards:\")\n",
        "    print_values(grid.rewards, grid)\n",
        "\n",
        "    # Value Iteration loop\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        biggest_change = 0  # Track the maximum change in V for convergence check\n",
        "        for s in grid.all_states():\n",
        "            if not grid.is_terminal(s):\n",
        "                old_v = V[s]  # Store old value for convergence check\n",
        "                new_v = float('-inf')  # Initialize with negative infinity for max operation\n",
        "\n",
        "                # Bellman optimality update for each action\n",
        "                for a in ACTION_SPACE:\n",
        "                    v = 0\n",
        "                    if (s, a) in transition_probs:\n",
        "                        # Calculate expected value of taking action a in state s\n",
        "                        for s2, prob in transition_probs[(s, a)].items():\n",
        "                            r = rewards.get((s, a, s2), 0)\n",
        "                            v += prob * (r + gamma * V[s2])\n",
        "                        new_v = max(new_v, v)  # Take the max over actions\n",
        "\n",
        "                V[s] = new_v  # Update value for the state\n",
        "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "        # Optionally print the value function after each iteration\n",
        "        if show_iteration:\n",
        "            print(f\"Iteration {iteration}: Value Function\")\n",
        "            print_values(V, grid)\n",
        "\n",
        "        # Stop if the value function has converged\n",
        "        if biggest_change < convergence_threshold:\n",
        "            break\n",
        "        iteration += 1\n",
        "\n",
        "    # Derive the optimal policy from the converged value function\n",
        "    policy = {}\n",
        "    for s in grid.actions.keys():\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "\n",
        "        # Find the best action for each state based on V\n",
        "        for a in ACTION_SPACE:\n",
        "            v = 0\n",
        "            if (s, a) in transition_probs:\n",
        "                # Calculate expected value for taking action a in state s\n",
        "                for s2, prob in transition_probs[(s, a)].items():\n",
        "                    r = rewards.get((s, a, s2), 0)\n",
        "                    v += prob * (r + gamma * V[s2])\n",
        "                if v > best_value:\n",
        "                    best_value = v\n",
        "                    best_action = a  # Update best action\n",
        "\n",
        "        policy[s] = best_action  # Set the best action for the state\n",
        "\n",
        "    # Print final value function and optimal policy\n",
        "    print(\"Final Value Function:\")\n",
        "    print_values(V, grid)\n",
        "    print(\"Optimal Policy:\")\n",
        "    print_policy(policy, grid)\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Run value iteration\n",
        "if __name__ == '__main__':\n",
        "    gamma = 0.9  # Define the discount factor\n",
        "    # Run value iteration for both standard and windy grids\n",
        "    print(\"Running Value Iteration on Windy Grid:\")\n",
        "    value_iteration(gamma, windy_grid, show_iteration=True)\n",
        "    print(\"Running Value Iteration on Standard Grid:\")\n",
        "    value_iteration(gamma, standard_grid, show_iteration=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhnlyyySJR6f",
        "outputId": "8d2b5ded-1035-4451-ca7a-6d0b6d423f91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Value Iteration on Windy Grid:\n",
            "Rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Iteration 0: Value Function\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Iteration 1: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Iteration 2: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Iteration 3: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.59| 0.53| 0.00|\n",
            "\n",
            "\n",
            "Iteration 4: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.48| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.59| 0.53| 0.48|\n",
            "\n",
            "\n",
            "Iteration 5: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.48| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.59| 0.53| 0.48|\n",
            "\n",
            "\n",
            "Final Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.48| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.59| 0.53| 0.48|\n",
            "\n",
            "\n",
            "Optimal Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  D  |     |\n",
            "---------------------------\n",
            "  U  |  L  |  L  |  L  |\n",
            "\n",
            "\n",
            "Running Value Iteration on Standard Grid:\n",
            "Rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Iteration 0: Value Function\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Iteration 1: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.81| 0.00|\n",
            "\n",
            "\n",
            "Iteration 2: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.73| 0.81| 0.73|\n",
            "\n",
            "\n",
            "Iteration 3: Value Function\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.73| 0.81| 0.73|\n",
            "\n",
            "\n",
            "Final Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.73| 0.81| 0.73|\n",
            "\n",
            "\n",
            "Optimal Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define possible actions in the environment\n",
        "ACTION_SPACE = ('U', 'D', 'L', 'R')  # U: Up, D: Down, L: Left, R: Right\n",
        "\n",
        "class Grid:  # Environment representing a grid world\n",
        "  def __init__(self, rows, cols, start):\n",
        "    \"\"\"\n",
        "    Initialize the grid environment with the given dimensions and starting position.\n",
        "    :param rows: Number of rows in the grid\n",
        "    :param cols: Number of columns in the grid\n",
        "    :param start: Starting position (i, j) for the agent\n",
        "    \"\"\"\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    \"\"\"\n",
        "    Set the rewards and possible actions for each cell in the grid.\n",
        "    :param rewards: Dictionary where keys are (i, j) positions and values are rewards\n",
        "    :param actions: Dictionary where keys are (i, j) positions and values are lists of allowed actions\n",
        "    \"\"\"\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    \"\"\"\n",
        "    Set the agent's position in the grid to a specific state.\n",
        "    :param s: Tuple (i, j) representing the agent's new position\n",
        "    \"\"\"\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    \"\"\"\n",
        "    Returns the current state (position) of the agent.\n",
        "    :return: Tuple (i, j) representing the agent's current position\n",
        "    \"\"\"\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    \"\"\"\n",
        "    Check if a state is terminal, meaning there are no actions available from this state.\n",
        "    :param s: Tuple (i, j) representing a state\n",
        "    :return: True if the state is terminal, False otherwise\n",
        "    \"\"\"\n",
        "    return s not in self.actions\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the agent to the starting position.\n",
        "    :return: Tuple (i, j) representing the agent's start position\n",
        "    \"\"\"\n",
        "    self.i = 2\n",
        "    self.j = 0\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def get_next_state(self, s, a):\n",
        "    \"\"\"\n",
        "    Get the next state given a current state and an action.\n",
        "    :param s: Current state (i, j) as a tuple\n",
        "    :param a: Action to be taken ('U', 'D', 'L', 'R')\n",
        "    :return: Tuple (new_i, new_j) representing the next state after taking action\n",
        "    \"\"\"\n",
        "    i, j = s[0], s[1]\n",
        "\n",
        "    # Update position based on the action, if the action is allowed from the current state\n",
        "    if a in self.actions[(i, j)]:\n",
        "      if a == 'U':\n",
        "        i -= 1\n",
        "      elif a == 'D':\n",
        "        i += 1\n",
        "      elif a == 'R':\n",
        "        j += 1\n",
        "      elif a == 'L':\n",
        "        j -= 1\n",
        "    return i, j\n",
        "\n",
        "  def move(self, action):\n",
        "    \"\"\"\n",
        "    Move the agent in the specified direction, if the action is allowed from the current position.\n",
        "    :param action: Action to take ('U', 'D', 'L', 'R')\n",
        "    :return: Reward received after taking the action\n",
        "    \"\"\"\n",
        "    # Check if the action is valid in the current state\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    # Return the reward for the new position, defaulting to 0 if no reward is defined\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    \"\"\"\n",
        "    Undo the last move, moving the agent in the opposite direction of the action.\n",
        "    :param action: Action to undo ('U', 'D', 'L', 'R')\n",
        "    \"\"\"\n",
        "    # Move in the opposite direction of the specified action\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    # Ensure the state after undoing is valid\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    \"\"\"\n",
        "    Check if the game is over, which is true if the agent is in a terminal state.\n",
        "    :return: True if in a terminal state, False otherwise\n",
        "    \"\"\"\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    \"\"\"\n",
        "    Get all possible states in the grid, defined as any position with actions or rewards.\n",
        "    :return: Set of tuples representing all possible states\n",
        "    \"\"\"\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid():\n",
        "  \"\"\"\n",
        "  Define a standard 3x4 grid with rewards and actions.\n",
        "  Layout:\n",
        "    .  .  .  1\n",
        "    .  x  . -1\n",
        "    s  .  .  .\n",
        "  Legend:\n",
        "    - s: Starting position\n",
        "    - x: Blocked position (no actions allowed)\n",
        "    - Numbers: Rewards at certain states\n",
        "\n",
        "  :return: An instance of the Grid class with rewards and actions set\n",
        "  \"\"\"\n",
        "  g = Grid(3, 4, (2, 0))  # 3x4 grid with start position at (2, 0)\n",
        "\n",
        "  # Define rewards for reaching specific states\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "\n",
        "  # Define possible actions from each state\n",
        "  actions = {\n",
        "    (0, 0): ('D', 'R'),  # Can go Down or Right from (0, 0)\n",
        "    (0, 1): ('L', 'R'),  # Can go Left or Right from (0, 1)\n",
        "    (0, 2): ('L', 'D', 'R'),  # Can go Left, Down, or Right from (0, 2)\n",
        "    (1, 0): ('U', 'D'),  # Can go Up or Down from (1, 0)\n",
        "    (1, 2): ('U', 'D', 'R'),  # Can go Up, Down, or Right from (1, 2)\n",
        "    (2, 0): ('U', 'R'),  # Can go Up or Right from (2, 0)\n",
        "    (2, 1): ('L', 'R'),  # Can go Left or Right from (2, 1)\n",
        "    (2, 2): ('L', 'R', 'U'),  # Can go Left, Right, or Up from (2, 2)\n",
        "    (2, 3): ('L', 'U'),  # Can go Left or Up from (2, 3)\n",
        "  }\n",
        "\n",
        "  # Set rewards and actions in the grid\n",
        "  g.set(rewards, actions)\n",
        "  return g"
      ],
      "metadata": {
        "id": "fdNmSWYvy2gY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convergence_threshold = 1e-3  # Threshold for convergence in policy evaluation\n",
        "\n",
        "# Define possible actions in the grid world\n",
        "ACTION_SPACE = ('U', 'D', 'L', 'R')  # Up, Down, Left, Right\n",
        "\n",
        "def print_values(V, g):\n",
        "    \"\"\"\n",
        "    Print the value function in a grid layout.\n",
        "    :param V: A dictionary mapping each state to its computed value.\n",
        "    :param g: The grid environment object.\n",
        "    \"\"\"\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            v = V.get((i, j), 0)  # Get the value for each state, defaulting to 0 if not in V\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")  # Align positive values with two decimal places\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\")  # Align negative values with two decimal places\n",
        "        print(\"\")\n",
        "    print(\"\\n\")  # Extra newline for better readability\n",
        "\n",
        "def print_policy(P, g):\n",
        "    \"\"\"\n",
        "    Print the policy in a grid layout.\n",
        "    :param P: A dictionary mapping each state to an action ('U', 'D', 'L', 'R').\n",
        "    :param g: The grid environment object.\n",
        "    \"\"\"\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            a = P.get((i, j), ' ')  # Get the action for each state, or blank if not in policy\n",
        "            print(\"  %s  |\" % a, end=\"\")\n",
        "        print(\"\")\n",
        "    print(\"\\n\")  # Extra newline for better readability\n",
        "\n",
        "def get_transition_probs_and_rewards(grid):\n",
        "    \"\"\"\n",
        "    Extract the transition probabilities and rewards for the grid.\n",
        "    :param grid: The grid environment object.\n",
        "    :return: A dictionary of transition probabilities and rewards for each state-action pair.\n",
        "    \"\"\"\n",
        "    transition_probs = {}  # Dictionary for transition probabilities\n",
        "    rewards = {}           # Dictionary for rewards\n",
        "\n",
        "    # Iterate over each cell in the grid\n",
        "    for i in range(grid.rows):\n",
        "        for j in range(grid.cols):\n",
        "            s = (i, j)\n",
        "            # Only define transitions for non-terminal states\n",
        "            if not grid.is_terminal(s):\n",
        "                for a in ACTION_SPACE:\n",
        "                    s2 = grid.get_next_state(s, a)  # Next state based on action\n",
        "                    transition_probs[(s, a, s2)] = 1  # Deterministic transition\n",
        "                    if s2 in grid.rewards:\n",
        "                        rewards[(s, a, s2)] = grid.rewards[s2]  # Assign reward if defined\n",
        "\n",
        "    return transition_probs, rewards\n",
        "\n",
        "def evaluate_deterministic_policy(grid, policy, transition_probs, rewards, gamma, initV=None):\n",
        "    \"\"\"\n",
        "    Evaluate a deterministic policy using iterative policy evaluation.\n",
        "    :param grid: The grid environment object.\n",
        "    :param policy: Dictionary mapping each state to the selected action under the policy.\n",
        "    :param transition_probs: Dictionary of transition probabilities for each state-action pair.\n",
        "    :param rewards: Dictionary of rewards for each state-action-next_state triplet.\n",
        "    :param gamma: Discount factor.\n",
        "    :param initV: Optional initial value function, used to start evaluation with a prior estimate.\n",
        "    :return: Final value function V after policy evaluation.\n",
        "    \"\"\"\n",
        "    # Initialize the value function V for all states, using initV if provided\n",
        "    V = initV if initV is not None else {s: 0 for s in grid.all_states()}\n",
        "\n",
        "    # Iterative policy evaluation loop\n",
        "    while True:\n",
        "        biggest_change = 0  # Track the largest change in V for convergence check\n",
        "        for s in grid.all_states():\n",
        "            if not grid.is_terminal(s):\n",
        "                old_v = V[s]\n",
        "                new_v = 0  # Calculate new value for state s\n",
        "                for a in ACTION_SPACE:\n",
        "                    for s2 in grid.all_states():\n",
        "                        action_prob = 1 if policy.get(s) == a else 0\n",
        "                        r = rewards.get((s, a, s2), 0)\n",
        "                        new_v += action_prob * transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
        "                V[s] = new_v  # Update value function\n",
        "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))  # Track largest update\n",
        "\n",
        "        # Stop if the value function has converged\n",
        "        if biggest_change < convergence_threshold:\n",
        "            break\n",
        "\n",
        "    return V  # Return the converged value function\n",
        "\n",
        "def deterministic_policy_iteration(gamma, initial_policy=\"random\", show_iteration=False):\n",
        "    \"\"\"\n",
        "    Perform policy iteration for the grid environment.\n",
        "    :param gamma: Discount factor.\n",
        "    :param initial_policy: Initial policy, either \"random\" or a predefined dictionary.\n",
        "    :param show_iteration: If True, show the policy and value function at each iteration.\n",
        "    :return: Final value function and policy after convergence.\n",
        "    \"\"\"\n",
        "    # Initialize the grid environment\n",
        "    grid = standard_grid()\n",
        "\n",
        "    # Get transition probabilities and rewards\n",
        "    transition_probs, rewards = get_transition_probs_and_rewards(grid)\n",
        "\n",
        "    # Initialize the policy\n",
        "    if initial_policy == \"random\":\n",
        "        policy = {s: np.random.choice(ACTION_SPACE) for s in grid.actions.keys()}\n",
        "    else:\n",
        "        policy = initial_policy\n",
        "\n",
        "    # Print rewards in the grid layout\n",
        "    print(\"Rewards:\")\n",
        "    print_values(grid.rewards, grid)\n",
        "\n",
        "    # Print initial policy layout\n",
        "    print(\"Initial Policy:\")\n",
        "    print_policy(policy, grid)\n",
        "\n",
        "    # Policy iteration loop\n",
        "    V = None  # Initialize value function V\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        # Optionally print iteration details\n",
        "        if show_iteration:\n",
        "            print(f\"Iteration {iteration}: Policy Improvement\")\n",
        "\n",
        "        # Policy evaluation step: evaluate the current policy\n",
        "        V = evaluate_deterministic_policy(grid, policy, transition_probs, rewards, gamma, initV=V)\n",
        "\n",
        "        # Optionally print the value function after evaluation\n",
        "        if show_iteration:\n",
        "            print(\"Value Function:\")\n",
        "            print_values(V, grid)\n",
        "\n",
        "        # Policy improvement step\n",
        "        is_policy_converged = True  # Track if policy has converged\n",
        "        for s in grid.actions.keys():\n",
        "            old_a = policy[s]  # Current action in the policy\n",
        "            new_a = None\n",
        "            best_value = float('-inf')  # Initialize best value for action selection\n",
        "\n",
        "            # Loop through all actions to find the best action under current V\n",
        "            for a in ACTION_SPACE:\n",
        "                v = 0\n",
        "                for s2 in grid.all_states():\n",
        "                    r = rewards.get((s, a, s2), 0)\n",
        "                    v += transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
        "\n",
        "                if v > best_value:\n",
        "                    best_value = v\n",
        "                    new_a = a\n",
        "\n",
        "            # Update the policy with the best action found\n",
        "            policy[s] = new_a\n",
        "            if new_a != old_a:\n",
        "                is_policy_converged = False  # If any action changes, policy is not yet converged\n",
        "\n",
        "        # Optionally print the policy after improvement step\n",
        "        if show_iteration:\n",
        "            print(\"Policy:\")\n",
        "            print_policy(policy, grid)\n",
        "\n",
        "        # Exit if policy has converged\n",
        "        if is_policy_converged:\n",
        "            break\n",
        "        iteration += 1\n",
        "\n",
        "    # Print final value function and policy after convergence\n",
        "    print(\"Final Value Function:\")\n",
        "    print_values(V, grid)\n",
        "    print(\"Final Policy:\")\n",
        "    print_policy(policy, grid)\n",
        "\n",
        "    return V, policy  # Return final value function and policy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gamma = 0.9  # Define the discount factor\n",
        "    # Run policy iteration with a random initial policy and display each iteration\n",
        "    deterministic_policy_iteration(gamma, initial_policy=\"random\", show_iteration=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYaZniohxwO3",
        "outputId": "fa19441c-51d2-4906-dfec-7ac9974c5c90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Initial Policy:\n",
            "---------------------------\n",
            "  R  |  D  |  U  |     |\n",
            "---------------------------\n",
            "  D  |     |  R  |     |\n",
            "---------------------------\n",
            "  D  |  R  |  L  |  L  |\n",
            "\n",
            "\n",
            "Iteration 0: Policy Improvement\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Policy:\n",
            "---------------------------\n",
            "  U  |  U  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  U  |  D  |  D  |\n",
            "\n",
            "\n",
            "Iteration 1: Policy Improvement\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Policy:\n",
            "---------------------------\n",
            "  U  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  U  |  U  |  D  |\n",
            "\n",
            "\n",
            "Iteration 2: Policy Improvement\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.00| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.81| 0.00|\n",
            "\n",
            "\n",
            "Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "\n",
            "\n",
            "Iteration 3: Policy Improvement\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.73| 0.81| 0.73|\n",
            "\n",
            "\n",
            "Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "\n",
            "\n",
            "Final Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.73| 0.81| 0.73|\n",
            "\n",
            "\n",
            "Final Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
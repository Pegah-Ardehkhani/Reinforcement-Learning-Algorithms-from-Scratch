{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a5xXBpPYP4bN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the possible actions\n",
        "ACTION_SPACE = ('U', 'D', 'L', 'R')  # Up, Down, Left, Right\n",
        "\n",
        "class WindyGrid:\n",
        "    def __init__(self, rows, cols, start):\n",
        "        \"\"\"\n",
        "        Initialize the grid with given dimensions and starting position.\n",
        "\n",
        "        Parameters:\n",
        "        rows (int): Number of rows in the grid.\n",
        "        cols (int): Number of columns in the grid.\n",
        "        start (tuple): Starting position (row, col).\n",
        "        \"\"\"\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.i = start[0]  # Row position of the agent\n",
        "        self.j = start[1]  # Column position of the agent\n",
        "\n",
        "    def set(self, rewards, actions, probs):\n",
        "        \"\"\"\n",
        "        Set the rewards, actions, and transition probabilities for each state.\n",
        "\n",
        "        Parameters:\n",
        "        rewards (dict): Dictionary mapping each state (i, j) to its reward.\n",
        "        actions (dict): Dictionary mapping each state (i, j) to available actions.\n",
        "        probs (dict): Dictionary defining the transition probabilities p(s' | s, a).\n",
        "        \"\"\"\n",
        "        self.rewards = rewards\n",
        "        self.actions = actions\n",
        "        self.probs = probs\n",
        "\n",
        "    def set_state(self, s):\n",
        "        \"\"\"\n",
        "        Set the agent's current state.\n",
        "\n",
        "        Parameters:\n",
        "        s (tuple): The state to set (i, j).\n",
        "        \"\"\"\n",
        "        self.i = s[0]\n",
        "        self.j = s[1]\n",
        "\n",
        "    def current_state(self):\n",
        "        \"\"\"\n",
        "        Return the agent's current state.\n",
        "\n",
        "        Returns:\n",
        "        tuple: The current state (i, j).\n",
        "        \"\"\"\n",
        "        return (self.i, self.j)\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        \"\"\"\n",
        "        Check if a given state is terminal (no actions available).\n",
        "\n",
        "        Parameters:\n",
        "        s (tuple): The state to check.\n",
        "\n",
        "        Returns:\n",
        "        bool: True if the state is terminal, False otherwise.\n",
        "        \"\"\"\n",
        "        return s not in self.actions\n",
        "\n",
        "    def move(self, action):\n",
        "        \"\"\"\n",
        "        Attempt to move the agent in the specified direction, using transition probabilities.\n",
        "\n",
        "        Parameters:\n",
        "        action (str): The action to take ('U', 'D', 'L', or 'R').\n",
        "\n",
        "        Returns:\n",
        "        int: The reward associated with the resulting state (if any).\n",
        "        \"\"\"\n",
        "        # Get the current state\n",
        "        s = (self.i, self.j)\n",
        "\n",
        "        # Get the transition probabilities for the action\n",
        "        next_state_probs = self.probs[(s, action)]\n",
        "        next_states = list(next_state_probs.keys())    # Possible next states\n",
        "        next_probs = list(next_state_probs.values())   # Corresponding probabilities\n",
        "\n",
        "        # Choose the next state based on the defined probabilities\n",
        "        next_state_idx = np.random.choice(len(next_states), p=next_probs)\n",
        "        s2 = next_states[next_state_idx]\n",
        "\n",
        "        # Update the agent's current position\n",
        "        self.i, self.j = s2\n",
        "\n",
        "        # Return the reward for the resulting state (if any)\n",
        "        return self.rewards.get(s2, 0)\n",
        "\n",
        "    def game_over(self):\n",
        "        \"\"\"\n",
        "        Check if the game is over (if the agent is in a terminal state).\n",
        "\n",
        "        Returns:\n",
        "        bool: True if the game is over, False otherwise.\n",
        "        \"\"\"\n",
        "        return (self.i, self.j) not in self.actions\n",
        "\n",
        "    def all_states(self):\n",
        "        \"\"\"\n",
        "        Get a set of all possible states, including states with rewards and states with available actions.\n",
        "\n",
        "        Returns:\n",
        "        set: A set containing all states in the grid.\n",
        "        \"\"\"\n",
        "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def windy_grid():\n",
        "    \"\"\"\n",
        "    Create a specific instance of the WindyGrid with predefined rewards, actions, and transition probabilities.\n",
        "\n",
        "    Returns:\n",
        "    WindyGrid: An instance of the WindyGrid class.\n",
        "    \"\"\"\n",
        "    # Initialize the grid with 3 rows, 4 columns, and a starting position at (2, 0)\n",
        "    g = WindyGrid(3, 4, (2, 0))\n",
        "\n",
        "    # Define rewards for certain states\n",
        "    rewards = {(0, 3): 1, (1, 3): -1}  # Positive reward at (0, 3), negative reward at (1, 3)\n",
        "\n",
        "    # Define actions available at each non-terminal state\n",
        "    actions = {\n",
        "        (0, 0): ('D', 'R'),\n",
        "        (0, 1): ('L', 'R'),\n",
        "        (0, 2): ('L', 'D', 'R'),\n",
        "        (1, 0): ('U', 'D'),\n",
        "        (1, 2): ('U', 'D', 'R'),\n",
        "        (2, 0): ('U', 'R'),\n",
        "        (2, 1): ('L', 'R'),\n",
        "        (2, 2): ('L', 'R', 'U'),\n",
        "        (2, 3): ('L', 'U'),\n",
        "    }\n",
        "\n",
        "    # Define transition probabilities p(s' | s, a)\n",
        "    # Each entry specifies possible outcomes when performing an action in a given state\n",
        "    probs = {\n",
        "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
        "        ((2, 0), 'D'): {(2, 0): 1.0},\n",
        "        ((2, 0), 'L'): {(2, 0): 1.0},\n",
        "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
        "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
        "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
        "        ((1, 0), 'L'): {(1, 0): 1.0},\n",
        "        ((1, 0), 'R'): {(1, 0): 1.0},\n",
        "        ((0, 0), 'U'): {(0, 0): 1.0},\n",
        "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
        "        ((0, 0), 'L'): {(0, 0): 1.0},\n",
        "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
        "        ((0, 1), 'U'): {(0, 1): 1.0},\n",
        "        ((0, 1), 'D'): {(0, 1): 1.0},\n",
        "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
        "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
        "        ((0, 2), 'U'): {(0, 2): 1.0},\n",
        "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
        "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
        "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
        "        ((2, 1), 'U'): {(2, 1): 1.0},\n",
        "        ((2, 1), 'D'): {(2, 1): 1.0},\n",
        "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
        "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
        "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
        "        ((2, 2), 'D'): {(2, 2): 1.0},\n",
        "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
        "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
        "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
        "        ((2, 3), 'D'): {(2, 3): 1.0},\n",
        "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
        "        ((2, 3), 'R'): {(2, 3): 1.0},\n",
        "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},  # Probabilistic transition\n",
        "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
        "        ((1, 2), 'L'): {(1, 2): 1.0},\n",
        "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
        "    }\n",
        "\n",
        "    # Set the rewards, actions, and transition probabilities in the grid\n",
        "    g.set(rewards, actions, probs)\n",
        "\n",
        "    # Return the configured grid instance\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "convergence_threshold = 1e-3  # Threshold for convergence\n",
        "\n",
        "def print_values(V, g, label=True):\n",
        "    if label:\n",
        "        print(\"Value Function:\")\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            v = V.get((i, j), 0)\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\")  # -ve sign takes up an extra space\n",
        "        print(\"\")\n",
        "    print(\"\\n\")  # Extra newline for better readability\n",
        "\n",
        "def print_policy(P, g):\n",
        "    \"\"\"\n",
        "    Print the policy for the grid. For each state, display the action(s) with probabilities.\n",
        "    If there's a single action with a 1.0 probability, display just that action.\n",
        "    \"\"\"\n",
        "    print(\"Policy:\")\n",
        "    for i in range(g.rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(g.cols):\n",
        "            actions = P.get((i, j), {})\n",
        "            if actions:\n",
        "                # If there's only one action with probability 1.0, display that action alone\n",
        "                if len(actions) == 1 and list(actions.values())[0] == 1.0:\n",
        "                    action = list(actions.keys())[0]\n",
        "                    print(\"  %s  |\" % action, end=\"\")\n",
        "                else:\n",
        "                    # Display all actions with their probabilities\n",
        "                    action_probs = \", \".join([f\"{a}:{p:.1f}\" for a, p in actions.items()])\n",
        "                    print(\" %s |\" % action_probs, end=\"\")\n",
        "            else:\n",
        "                # If no actions are defined, print an empty space or terminal state indicator\n",
        "                print(\"     |\", end=\"\")\n",
        "        print(\"\")\n",
        "    print(\"\\n\")  # Extra newline for better readability\n",
        "\n",
        "\n",
        "def Probabilistic_iterative_policy_evaluation(policy, gamma):\n",
        "    \"\"\"\n",
        "    Perform iterative policy evaluation for a given probabilistic policy and discount factor gamma.\n",
        "    :param policy: Dictionary mapping each state (i, j) to a dict of actions with probabilities\n",
        "    :param gamma: Discount factor\n",
        "    \"\"\"\n",
        "    # Initialize the grid environment\n",
        "    grid = windy_grid()\n",
        "\n",
        "    # Define transition probabilities and rewards from the grid\n",
        "    transition_probs = {}\n",
        "    rewards = {}\n",
        "    for (s, a), transitions in grid.probs.items():\n",
        "        for s2, prob in transitions.items():\n",
        "            transition_probs[(s, a, s2)] = prob\n",
        "            rewards[(s, a, s2)] = grid.rewards.get(s2, 0)\n",
        "\n",
        "    # Print the initial policy\n",
        "    print_policy(policy, grid)\n",
        "\n",
        "    # Initialize V(s) = 0 for all states\n",
        "    V = {s: 0 for s in grid.all_states()}\n",
        "\n",
        "    # Iterative Policy Evaluation\n",
        "    it = 0\n",
        "    while True:\n",
        "        biggest_change = 0\n",
        "        for s in grid.all_states():\n",
        "            if not grid.is_terminal(s):\n",
        "                old_v = V[s]\n",
        "                new_v = 0  # Accumulate value based on policy\n",
        "\n",
        "                # Iterate over all actions in the action space\n",
        "                for a in ACTION_SPACE:\n",
        "                    # Probability of taking action `a` under the policy for state `s`\n",
        "                    action_prob = policy.get(s, {}).get(a, 0)\n",
        "\n",
        "                    # Sum up for each possible next state `s2`\n",
        "                    for s2 in grid.all_states():\n",
        "                        # Reward for (s, a, s')\n",
        "                        r = rewards.get((s, a, s2), 0)\n",
        "\n",
        "                        # Calculate the expected value\n",
        "                        new_v += action_prob * transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
        "\n",
        "                # Update the value function\n",
        "                V[s] = new_v\n",
        "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "        # Print iteration results\n",
        "        print(f\"Iteration: {it}, Biggest Change: {biggest_change:.6f}\")\n",
        "        print_values(V, grid)\n",
        "\n",
        "        # Check for convergence\n",
        "        it += 1\n",
        "        if biggest_change < convergence_threshold:\n",
        "            break\n",
        "\n",
        "    # Final print for the converged value function\n",
        "    print(\"Final Value Function:\")\n",
        "    print_values(V, grid, label=False)  # Call without label\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define a probabilistic policy for testing\n",
        "    policy = {\n",
        "        (2, 0): {'U': 0.5, 'R': 0.5},\n",
        "        (1, 0): {'U': 1.0},\n",
        "        (0, 0): {'R': 1.0},\n",
        "        (0, 1): {'R': 1.0},\n",
        "        (0, 2): {'R': 1.0},\n",
        "        (1, 2): {'U': 1.0},\n",
        "        (2, 1): {'R': 1.0},\n",
        "        (2, 2): {'U': 1.0},\n",
        "        (2, 3): {'L': 1.0},\n",
        "    }\n",
        "    gamma = 0.9  # Discount factor\n",
        "\n",
        "    # Run iterative policy evaluation\n",
        "    Probabilistic_iterative_policy_evaluation(policy, gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FainRLcLToyg",
        "outputId": "d6a78406-4a87-4ad4-fdc5-46423fb1eda9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            " U:0.5, R:0.5 |  R  |  U  |  L  |\n",
            "\n",
            "\n",
            "Iteration: 0, Biggest Change: 1.000000\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-0.50| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-0.45| 0.00|\n",
            "\n",
            "\n",
            "Iteration: 1, Biggest Change: 0.900000\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            "-0.18|-0.41|-0.04|-0.41|\n",
            "\n",
            "\n",
            "Iteration: 2, Biggest Change: 0.492075\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            " 0.31|-0.04|-0.04|-0.04|\n",
            "\n",
            "\n",
            "Iteration: 3, Biggest Change: 0.000000\n",
            "Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            " 0.31|-0.04|-0.04|-0.04|\n",
            "\n",
            "\n",
            "Final Value Function:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            " 0.31|-0.04|-0.04|-0.04|\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}